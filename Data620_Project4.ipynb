{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Project4\n",
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  UCI Machine Learning Repository: Spambase Data Set\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.\n",
    "Also, you are free to use other machine learning libraries like scikit-learn and use their classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Dataset\n",
    "http://www2.aueb.gr/users/ion/data/enron-spam/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "import re #regular expression\n",
    "import glob\n",
    "\n",
    "#sklearn\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Used\n",
    "get_documents is a function that takes a path to a folder that contains files and a value of 1 or 0 that specifies if the documents in the folder are spam or not. \n",
    "The function expects that all spam messages are contained in a single folder and all ham messages are contained in a single folder as well. \n",
    "The expected encoding is \"utf8\". If an error errors while reading a document, the error is ignored and the next document is read.\n",
    "\n",
    "folerpath: path to the folder\n",
    "spam_value: 1 if the messages in the folder are spam. 0 if the messages in the folder are not spam.\n",
    "This function returns a data frame of messages with the columns 'spam' and 'message'.\n",
    "\n",
    "get_alphawords is a function that takes a string and extracts words with alpha characters. Trailing and leading spaces are removed. English stop words are removed. The words are stemmed. A string is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folderpath: relative path to the folder that contains texts classified as either spam or ham\n",
    "#spam_value: pass '1' if documents in folder are spam; pass '0' if documents in folder are ham.\n",
    "#this function skips any documents that generates errors due to unrecognized encoding\n",
    "def get_documents(folderpath, spam_value):\n",
    "    filepath_list = glob.glob(folderpath)\n",
    "    d = list()\n",
    "    for filepath in filepath_list:\n",
    "        file_object  = open(filepath, \"r\", encoding=\"utf8\", errors='ignore')\n",
    "        message = file_object.read()\n",
    "        document = {'spam':[spam_value], 'message': [message]}\n",
    "        d.append(pd.DataFrame(document))\n",
    "    return pd.concat(d) #return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract only alpha characters, remove english stopwords, and stem words\n",
    "def get_alphawords(message):\n",
    "    message = message.lower()\n",
    "    #only look at alpha characters\n",
    "    thispattern = \"[a-z]+ |[a-z]+[a-z]$|[a-z]+[\\\\.|,|;] \"\n",
    "    alpha_words_list = re.findall(thispattern, message)\n",
    "    words_strip = [word.strip() for word in alpha_words_list] #remove leading/trailing spaces\n",
    "    sw = stopwords.words('english')\n",
    "    words = [word for word in words_strip if word not in sw] #remove stop words\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join([word for word in stemmed_words]) #return a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Ham & Spam Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data = get_documents('/Users/rajans/Desktop/CUNY/Data620/Project 4/Enron_Spam/spam/*', 1) \n",
    "ham_data = get_documents('/Users/rajans/Desktop/CUNY/Data620/Project 4/Enron_Spam/ham/*', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 2)\n",
      "(3672, 2)\n"
     ]
    }
   ],
   "source": [
    "print(spam_data.shape)\n",
    "print(ham_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Size of spam and ham\n",
    "There are 1500 spam documents and 3672 ham documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview documents\n",
    "Preview of a single document before clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: what up , , your cam babe\\nwhat are y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: want to make more money ?\\norder conf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: food for thoughts\\n[\\njoin now - take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: miningnews . net newsletter - tuesday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: your pharmacy ta\\nwould you want chea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                            message\n",
       "0     1  Subject: what up , , your cam babe\\nwhat are y...\n",
       "0     1  Subject: want to make more money ?\\norder conf...\n",
       "0     1  Subject: food for thoughts\\n[\\njoin now - take...\n",
       "0     1  Subject: miningnews . net newsletter - tuesday...\n",
       "0     1  Subject: your pharmacy ta\\nwould you want chea..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: ena sales on hpl\\njust to update you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: 98 - 6736 &amp; 98 - 9638 for 1997 ( ua 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: hpl nominations for december 28 , 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: revised nom - kcs resources\\ndaren ,\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: new production - sitara deals needed\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                            message\n",
       "0     0  Subject: ena sales on hpl\\njust to update you ...\n",
       "0     0  Subject: 98 - 6736 & 98 - 9638 for 1997 ( ua 4...\n",
       "0     0  Subject: hpl nominations for december 28 , 199...\n",
       "0     0  Subject: revised nom - kcs resources\\ndaren ,\\...\n",
       "0     0  Subject: new production - sitara deals needed\\..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Subject: what up , , your cam babe\\nwhat are you looking for ?\\nif your looking for a companion for friendship , love , a date , or just good ole '\\nfashioned * * * * * * , then try our brand new site ; it was developed and created\\nto help anyone find what they ' re looking for . a quick bio form and you ' re\\non the road to satisfaction in every sense of the word . . . . no matter what\\nthat may be !\\ntry it out and youll be amazed .\\nhave a terrific time this evening\\ncopy and pa ste the add . ress you see on the line below into your browser to come to the site .\\nhttp : / / www . meganbang . biz / bld / acc /\\nno more plz\\nhttp : / / www . naturalgolden . com / retract /\\ncounterattack aitken step preemptive shoehorn scaup . electrocardiograph movie honeycomb . monster war brandywine pietism byrne catatonia . encomia lookup intervenor skeleton turn catfish .\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spam_data.iloc[0]['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract words with letters only\n",
    "The function get_alphawords is used to extract words with alpha characters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data['message_alphawords'] = spam_data['message'].apply(lambda x: get_alphawords(x))\n",
    "ham_data['message_alphawords'] = ham_data['message'].apply(lambda x: get_alphawords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5172"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spam_data.append(ham_data)\n",
    "data = data.drop(['message'], axis=1)\n",
    "data.columns = ['spam', 'message']\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining spam and ham data into a single data frame 'data'. There are, in total, 5172 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'net tue sep localhost jalapeno org esmtp id tue, sep jalapeno localhost imap localhost tue, sep net net org esmtp id tue, sep net net net smtp id tue, sep copyyourdvd friend, copi dvd playstat game tue, sep ipa aug alternative; plain; copi backup dvd r software, dvd playstat game. never buy anoth backup dvd movi again. copi first time softwar avail public. softwar need burn dvd video, includ softwar packag dvd copi movi play standard dvd player. detailed, easi follow, step instructions, burn dvd video use noth rom r drives. purchas click order today step step interact softwar tool includ dvd burner free live technic day risk free trial dvd copi plu backup dvd movi min min use past creat audio s. softwar compress larg dvd file standard dvd vcd, svcd, divx much way popular format compress audio. order today start thank take privaci serious polici never unwant email messages. messag sent origin join one member site sign parti contract atomicdot. unsubscrib repli email unsubscrib html; backup meta type html; meta mshtml blockquot px; px; px; px solid; tabl center ffffff td img td img td img td img td img map td gif td top left font verdana, arial, helvetica, dvdcopyplu copi burn font font verdana, arial, helvetica, dvd font font verdana, arial, helvetica, playstat font font verdana, arial, helvetica, s, avi multimedia font font verdana, arial, helvetica, software, music cd perfect raw data font font verdana, arial, helvetica, burn r font verdana, arial, helvetica, protect invest back cd, dvd, data game collect burn copi realli separately, dvdcopyplu worth softwar purchas bundl right p font verdana, arial, helvetica, font order td gif td img td map area poli font serif font take privaci serious polici never send unwant email messages. messag sent com origin join one member site sign parti contract atomicdot. pleas click unsubscrib font repli email unsubscrib img font'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 'x' and 'y'\n",
    "The sklearn library is used to create the model that classifies documents are either spam or not spam.\n",
    "\n",
    "The x data is the text of the document (independent variable).\n",
    "The y data is the spam status of 1 or 0 (dependent variable). The y variable is what the model is going to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = data['message']\n",
    "df_y = data['spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test\n",
    "The function train_test_split of sklearn.model_selection is used to split the data into random train and test subsets. The test size is set to 25%.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Training data - 75%\n",
    "\n",
    "x_train - message data used to train the model (independent)\n",
    "x_test - corresponding spam status used to train the model (dependent)\n",
    "Test data - 25%\n",
    "\n",
    "y_train - message data used to test predictions of model (independent)\n",
    "y_test - corresponding actual spam status that model attempts to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1293\n",
      "1293\n",
      "3879\n",
      "3879\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.25, random_state=4)\n",
    "\n",
    "print(len(x_test))\n",
    "print(len(y_test))\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "Sklearn's TF-IDF vectorizer function was used to generate the term frequency-inverse document frequency value for each of the word in the training and test message data.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "min_df: float in range [0.0, 1.0] or int (default=1) When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "The line of code below create a TF-IDF vectorizer object. This vectorizer removes stop words in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a tf-idf vectorizer\n",
    "tfidf_vec = TfidfVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of what fit_transform does:\n",
    "\n",
    "https://datascience.stackexchange.com/questions/12321/difference-between-fit-and-fit-transform-in-scikit-learn-models\n",
    "\n",
    "The fit_transform is run on te train 'x' data. This calculates the mean and standard deviations, which are used to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf = tfidf_vec.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29322\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 29,322 features in the train set (unique words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview of feature names in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaa',\n",
       " 'aabda',\n",
       " 'aabvmmq',\n",
       " 'aac',\n",
       " 'aachecar',\n",
       " 'aafco',\n",
       " 'aaiab',\n",
       " 'aaigrcrb',\n",
       " 'aaihmqv',\n",
       " 'aaldano',\n",
       " 'aalland',\n",
       " 'aambiqu',\n",
       " 'aamlrg',\n",
       " 'aaoeuro',\n",
       " 'aar',\n",
       " 'aarhu',\n",
       " 'aaron',\n",
       " 'aavil',\n",
       " 'aaxrzm']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.get_feature_names()[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A demo of the tfidf output\n",
    "The tfidf vectorizer output is an array of list that gives the tfidf value of the words in the tfidf_vec.get_feature_names(). These feature names are the 'columns names' of the array of list. Each list in the array represents a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a = x_train_tfidf.toarray()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3879, 29322)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tfidf output has 3879 rows (number of documents in the train set) and 29,322 columns (number of distinct terms or words in the entire train set). Each value in the array is a tfidf score of the corresponding term in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse_transform function maps an array element back to the words in the document.\n",
    "\n",
    "The example below selects the tf-idf output of the first document in the train set, a[0], and passes this to the inverse_transform function. The x_train.iloc[0] shows you the content of the message of the first document in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['ababa', 'aberr', 'acid', 'alistair', 'alli', 'amplifi', 'ankara',\n",
       "        'anorthosit', 'arden', 'arduou', 'asplenium', 'astrophysicist',\n",
       "        'backpack', 'batik', 'befoul', 'beliz', 'berkeley', 'bingham',\n",
       "        'blackberri', 'brainchildren', 'bze', 'calcin', 'calder',\n",
       "        'canterburi', 'cheerlead', 'citi', 'codi', 'commot', 'conclav',\n",
       "        'coney', 'cormor', 'decolletag', 'diffus', 'disastrouscornm',\n",
       "        'dissembl', 'divers', 'divis', 'doll', 'dragonhead', 'email',\n",
       "        'emil', 'endur', 'flagstaff', 'fpc', 'gadolinium', 'gala',\n",
       "        'gallup', 'gigaherz', 'greensboro', 'greg', 'gullibl',\n",
       "        'harpydusti', 'hawk', 'hesitat', 'historian', 'horac', 'humpback',\n",
       "        'hygromet', 'import', 'incid', 'instal', 'insupport', 'intern',\n",
       "        'irresolv', 'janissari', 'kitti', 'lemma', 'lemon', 'lieuten',\n",
       "        'literari', 'locutor', 'manserv', 'midrang', 'minuet', 'misshapen',\n",
       "        'molin', 'monomer', 'motley', 'mueller', 'nordstrom', 'north',\n",
       "        'nuclei', 'octenni', 'optoelectron', 'padtestb', 'parquet',\n",
       "        'pentagram', 'phentermin', 'planoconvex', 'playground', 'potboil',\n",
       "        'pyrex', 'quad', 'queasi', 'remov', 'respit', 'reynold',\n",
       "        'rhapsodi', 'ripoff', 'sack', 'sanction', 'save', 'scarfac',\n",
       "        'sedg', 'sidewis', 'singlehand', 'situat', 'solidifi', 'speakeasi',\n",
       "        'spit', 'spruce', 'st', 'supersav', 'synthesi', 'teach', 'teammat',\n",
       "        'termin', 'thereund', 'tibet', 'transship', 'ubiqu', 'valium',\n",
       "        'wand', 'whoop', 'william', 'xanax', 'yard'], dtype='<U24')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.inverse_transform(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'supersav phentermin save xanax valium phentermin email remov go dissembl incid hawk ababa alistair gigaherz bingham cormor flagstaff arden planoconvex emil doll lemma berkeley codi canterburi bze intern import north front st beliz citi ankara dragonhead diffus decolletag humpback parquet divis acid motley teammat gullibl midrang irresolv calder astrophysicist instal amplifi endur greg misshapen gallup nordstrom sanction janissari monomer sack horac kitti lieuten spit cheerlead greensboro playground singlehand speakeasi scarfac commot spruce hesitat octenni pyrex hygromet insupport sedg situat respit ripoff backpack sidewis alli disastrouscornm minuet quad mueller optoelectron fpc potboil teach might whoop coney arduou synthesi tibet manserv solidifi harpydusti termin locutor thereund wand almost nuclei asplenium queasi brainchildren batik historian blackberri rhapsodi befoul calcin conclav padtestb gala aberr literari ubiqu yard gadolinium transship lemon anorthosit reynold william pentagram divers molin'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes\n",
    "Multinomail Navie Bayes is used to create the spam classifier.\n",
    "\n",
    "The MultinomialNB class of sklearn.naive_bayes library is used to create this object.\n",
    "\n",
    "The line of code below creates a Multinomial Naive Bayes object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit x, y data with multinomial naive bayes\n",
    "About MultinomialNB fit:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB.fit\n",
    "\n",
    "The x_train's tfidf array is passed along with the target values. The fit is done with alpha=1.0.\n",
    "\n",
    "Choosing parameter alpha :\n",
    "\n",
    "https://stackoverflow.com/questions/33830959/multinomial-naive-bayes-parameter-alpha-setting-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the 'transform' function on the test data\n",
    "The tfidf vectorizer object's transform function is used for the test data. It's important that the 'fit_transform' function is NOT used for this purpose. The transform function uses the same mean and standard deviation used for the train set. If the 'fit_transform' function is used on the test set, this will generate a new mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1293\n",
      "(1293, 29322)\n"
     ]
    }
   ],
   "source": [
    "x_test_tfidf = tfidf_vec.transform(x_test)\n",
    "print(len(x_test))\n",
    "print(x_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29322"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict spam status of test data\n",
    "The MultinomialNB's predict function is used to predict the spam status of the test data based on the multinomial naive bayes model that was built earlier with the train data. The predict function takes the tfidf data of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1293, 29322)\n",
      "1293\n"
     ]
    }
   ],
   "source": [
    "print(x_test_tfidf.shape)\n",
    "predict = mnb.predict(x_test_tfidf)\n",
    "print(len(predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare predicted and actual spam status of test data\n",
    "The y_test variable holds the actual spam status of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1293"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = np.array(y_test)\n",
    "len(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below compares the spam status of each document in the test set and increments count by 1 if the predicted spam status matches the actual spam status of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(predict)):\n",
    "    if predict[i] == actual[i]:\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1293\n",
      "1293\n",
      "1187\n"
     ]
    }
   ],
   "source": [
    "print(len(predict))\n",
    "print(len(actual))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9180201082753287"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count/len(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are 1,293 documents in the test set, and the model correctly classified 1,187 of these documents.\n",
    "\n",
    "This model has a 91.80% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397\n",
      "896\n",
      "292\n",
      "895\n"
     ]
    }
   ],
   "source": [
    "print((actual == 1).sum())\n",
    "\n",
    "print((actual == 0).sum())\n",
    "\n",
    "print((predict[np.isin(actual, [1])] == 1).sum())\n",
    "\n",
    "print((predict[np.isin(actual, [0])] == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "397 documents in test set that are spam\n",
    "896 documens in test set that are not spam \n",
    "292 out of 397 spam documents were correctly classified as spam\n",
    "895 out of 896 ham documents were correctly classified as not spam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7355163727959698\n",
      "0.9988839285714286\n"
     ]
    }
   ],
   "source": [
    "#correctly classified spam\n",
    "print(292/397)\n",
    "\n",
    "#correctly classified not spam\n",
    "print(895/896)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "About 73.55% of spam documents were correctly classified.\n",
    "About 99.88% of not spam documents were correctly classsified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data to plot\n",
    "n_groups = 2\n",
    "actual_status = (397, 896)\n",
    "predicted_status = (292, 895)\n",
    " \n",
    "# create plot\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.40\n",
    "opacity = 0.6\n",
    " \n",
    "rects1 = plt.bar(index, actual_status, bar_width,\n",
    "alpha=opacity,\n",
    "color='green',\n",
    "label='Actual')\n",
    " \n",
    "rects2 = plt.bar(index + bar_width, predicted_status, bar_width,\n",
    "alpha=opacity,\n",
    "color='blue',\n",
    "label='Predicted')\n",
    " \n",
    "plt.xlabel('Test Set')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.xticks(index + bar_width, ('Spam', 'Not Spam'))\n",
    "plt.legend()\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "Tutorial on Sklearn Vectorizer and Multinomial Naive Bayes\n",
    "\n",
    "CountVectorizer: https://youtu.be/RZYjsw6P4nI\n",
    "TFIDF: https://www.youtube.com/watch?v=bPYJi1E9xeM\n",
    "Helpful information on term frequency-inverse document frequency\n",
    "\n",
    "https://towardsdatascience.com/spam-classifier-in-python-from-scratch-27a98ddd8e73"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
